from abc import ABC, abstractmethod
from typing import Any
import numpy as np

METRICS = [
    "mean_squared_error",
    "accuracy",
    "precision_macro",
    "recall_macro",
    "mean_absolute_error",
    "r_squared"
] # add the names (in strings) of the metrics you implement

class Metric(ABC):
    """
    Abstract base class for all metrics.
    """
    # your code here
    # remember: metrics take ground truth and prediction as input and return a real number
    _type: str
    _name: str

    @property
    def name(self)->str:
        return self._name

    @property
    def type(self)->str:
        """
        Return a metric's type.

        Returns:
            str: metric's type
        """

        return self._type

    @staticmethod
    @abstractmethod
    def evaluate(self, predictions: list, actual: list) -> float:
        """
        Evaluate the predictions based on the given ground truth.

        Args:
            predictions: ???? the predictions generated by our model.
            actual: ???? the actual grounbd truth.

        Returns:
            float number representing our evaluated metric.
        """
        pass

    def __call__(self, predictions, actual) -> float:
        return self.evaluate(predictions, actual)

# add here concrete implementations of the Metric class

class MeanSquaredError(Metric):
    """
    Class representing the mean-squared-error metric.
    """
    _type:str = "numerical"
    name:str = "mean_squared_error"

    @staticmethod
    def evaluate(predictions: np.ndarray, actual: np.ndarray) -> float:
        """
        Evaluate the predictions based on the given ground truth.

        Args:
            predictions: ???? the predictions generated by our model.
            actual: ???? the actual grounbd truth.

        Returns:
            float number representing our evaluated metric.
        """
        n = len(predictions)
        sum = 0

        for i in range(n):
            sum += (predictions[i] - actual[i])**2

        return sum/n

class Accuracy(Metric):
    """
    Class representing the accuracy metric.
    """

    _type:str = "categorical"
    name: str = "accuracy"

    @staticmethod
    def evaluate(predictions: np.ndarray, actual: np.ndarray):
        n = len(predictions)
        sum = 0

        for i in range(n):
            sum += int(predictions[i] == actual[i])

        return sum/n


class PrecisionMacro(Metric):
    """
    Class representing the macro precision metric.
    """

    _type:str = "categorical"
    name:str = "precision_macro"

    @staticmethod
    def evaluate(predictions: np.ndarray, actual: np.ndarray):

        categories = np.unique(actual)
        macro_sum = 0
        for category in categories:
            TP = 0
            FP = 0
            for i in range(len(predictions)):
                if category == predictions[i]:
                    if predictions[i] == actual[i]:
                        TP += 1
                    else:
                        FP += 1
            macro_sum += TP/(TP+FP)

        return macro_sum/len(categories)

class RecallMacro(Metric):
    """
    Class representing the macro recall metric.
    """

    _type:str = "categorical"
    name:str = "recall_macro"

    @staticmethod
    def evaluate(predictions: np.ndarray, actual: np.ndarray):


        categories = np.unique(actual)
        macro_sum = 0
        for category in categories:
            TP = 0
            FN = 0
            for i in range(len(predictions)):
                if category == predictions[i]:
                    if predictions[i] == actual[i]:
                        TP += 1
                else:
                    if predictions[i] != actual[i]:
                        FN += 1

            macro_sum += TP/(TP+FN)

        return macro_sum/len(categories)

class MeanAbsoluteError(Metric):
    """
    Class representing the mean metric.
    """

    _type:str = "numerical"
    name:str = "mean_absolute_error"

    @staticmethod
    def evaluate(predictions: np.ndarray, actual: np.ndarray) -> float:
        """
        Evaluate the predictions based on the given ground truth.

        Args:
            predictions: ???? the predictions generated by our model.
            actual: ???? the actual grounbd truth.

        Returns:
            float number representing our evaluated metric.
        """
        n = len(predictions)
        sum = 0

        for i in range(n):
            sum += abs(predictions[i] - actual[i])

        return sum/n


class RSquared(Metric):
    """
    Class representing the R-squared error.
    """

    _type:str = "numerical"
    name:str = "r_squared"

    @staticmethod
    def evaluate(predictions: np.ndarray, actual: np.ndarray) -> float:
        mean = np.mean(actual)

        sum_denominator = 0
        sum_divider = 0

        for i in range(len(predictions)):
            sum_denominator += (actual[i] - predictions[i])**2
            sum_divider += (actual[i] - mean)**2

        return 1-(sum_denominator/sum_divider)


def get_metric(name: str) -> Metric:
    # Factory function to get a metric by name.
    # Return a metric instance given its str name.
    match name:
        case "mean_squared_error":
            return MeanSquaredError()
        case "accuracy":
            return Accuracy()
        case "precision_macro":
            return PrecisionMacro()
        case "recall_macro":
            return RecallMacro()
        case "r_squared":
            return RSquared()
        case "mean_absolute_error":
            return MeanAbsoluteError()
        case _:
            raise KeyError("NO SUCH METRIC FOUND")