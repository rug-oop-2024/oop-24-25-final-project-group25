from abc import ABC, abstractmethod
import numpy as np

# list holding all metric names
METRICS = [
    "mean_squared_error",
    "accuracy",
    "precision_macro",
    "recall_macro",
    "mean_absolute_error",
    "r_squared",
]


class Metric(ABC):
    """
    Abstract base class for all metrics.

    Attributes:
        _type(str): 'numerical' or 'categorical', depending on the metric's
            supported type
        _name: metric's name
    """

    _type: str
    _name: str

    @property
    def name(self) -> str:
        """
        Return metric's name.

        Returns:
            str: metric's name
        """

        return self._name

    @property
    def type(self) -> str:
        """
        Return a metric's type.

        Returns:
            str: metric's type
        """

        return self._type

    @staticmethod
    @abstractmethod
    def evaluate(self, predictions: np.ndarray, actual: np.ndarray) -> float:
        """
        Evaluate the predictions based on the given ground truth.

        Args:
            predictions: the predictions generated by our model.
            actual: the actual ground truth.

        Returns:
            float: number representing our evaluated metric.
        """
        pass

    def __str__(self) -> str:
        """
        Return the name of the metric.

        Returns:
            str: metric's name
        """
        return self._name

    def __call__(self, predictions, actual) -> float:
        """
        Describe metric's behaviour as a callable: evaluate the given input.

        Args:
            predictions: predictions to evaluate
            actual: actrual ground truth to evaluate by

        Returns:
            float: number representing our evaluated metric.
        """
        return self.evaluate(predictions, actual)


class MeanSquaredError(Metric):
    """
    Class representing the mean-squared-error metric.

    Attributes:
        _type(str): 'numerical', the metric's
        _name: metric's name
    """

    _type: str = "numerical"
    _name: str = "mean_squared_error"

    @staticmethod
    def evaluate(predictions: np.ndarray, actual: np.ndarray) -> float:
        """
        Evaluate the predictions based on the given ground truth.

        Args:
            predictions: the predictions generated by our model.
            actual: the actual ground truth.

        Returns:
            float number representing our evaluated metric.
        """
        n = len(predictions)
        sum = 0

        for i in range(n):
            sum += (predictions[i] - actual[i]) ** 2

        return sum / n


class Accuracy(Metric):
    """
    Class representing the accuracy metric.

    Attributes:
        _type(str): 'categorical', the metric's
        _name: metric's name
    """

    _type: str = "categorical"
    _name: str = "accuracy"

    @staticmethod
    def evaluate(predictions: np.ndarray, actual: np.ndarray) -> float:
        """
        Evaluate the predictions based on the given ground truth.

        Args:
            predictions: the predictions generated by our model.
            actual: the actual ground truth.

        Returns:
            float number representing our evaluated metric.
        """
        n = len(predictions)
        sum = 0

        for i in range(n):
            sum += int(predictions[i] == actual[i])

        return sum / n


class PrecisionMacro(Metric):
    """
    Class representing the macro precision metric.

    Attributes:
        _type(str): 'categorical', the metric's
        _name: metric's name
    """

    _type: str = "categorical"
    _name: str = "precision_macro"

    @staticmethod
    def evaluate(predictions: np.ndarray, actual: np.ndarray) -> float:
        """
        Evaluate the predictions based on the given ground truth.

        Args:
            predictions: the predictions generated by our model.
            actual: the actual ground truth.

        Returns:
            float number representing our evaluated metric.
        """

        categories = np.unique(actual)
        macro_sum = 0
        for category in categories:
            TP = 0
            FP = 0
            for i in range(len(predictions)):
                if category == predictions[i]:
                    if predictions[i] == actual[i]:
                        TP += 1
                    else:
                        FP += 1
            macro_sum += TP / (TP + FP)

        return macro_sum / len(categories)


class RecallMacro(Metric):
    """
    Class representing the macro recall metric.

    Attributes:
        _type(str): 'categorical', the metric's
        _name: metric's name
    """

    _type: str = "categorical"
    _name: str = "recall_macro"

    @staticmethod
    def evaluate(predictions: np.ndarray, actual: np.ndarray) -> float:
        """
        Evaluate the predictions based on the given ground truth.

        Args:
            predictions: the predictions generated by our model.
            actual: the actual ground truth.

        Returns:
            float number representing our evaluated metric.
        """

        categories = np.unique(actual)
        macro_sum = 0
        for category in categories:
            TP = 0
            FN = 0
            for i in range(len(predictions)):
                if category == predictions[i]:
                    if predictions[i] == actual[i]:
                        TP += 1
                else:
                    if predictions[i] != actual[i]:
                        FN += 1

            macro_sum += TP / (TP + FN)

        return macro_sum / len(categories)


class MeanAbsoluteError(Metric):
    """
    Class representing the mean metric.

    Attributes:
        _type(str): 'numerical', the metric's
        _name: metric's name
    """

    _type: str = "numerical"
    _name: str = "mean_absolute_error"

    @staticmethod
    def evaluate(predictions: np.ndarray, actual: np.ndarray) -> float:
        """
        Evaluate the predictions based on the given ground truth.

        Args:
            predictions: the predictions generated by our model.
            actual: the actual ground truth.

        Returns:
            float number representing our evaluated metric.
        """
        n = len(predictions)
        sum = 0

        for i in range(n):
            sum += abs(predictions[i] - actual[i])

        return sum / n


class RSquared(Metric):
    """
    Class representing the R-squared error.

    Attributes:
        _type(str): 'numerical', the metric's
        _name: metric's name
    """

    _type: str = "numerical"
    _name: str = "r_squared"

    @staticmethod
    def evaluate(predictions: np.ndarray, actual: np.ndarray) -> float:
        """
        Evaluate the predictions based on the given ground truth.

        Args:
            predictions: the predictions generated by our model.
            actual: the actual ground truth.

        Returns:
            float number representing our evaluated metric.
        """

        mean = np.mean(actual)

        sum_denominator = 0
        sum_divider = 0

        for i in range(len(predictions)):
            sum_denominator += (actual[i] - predictions[i]) ** 2
            sum_divider += (actual[i] - mean) ** 2

        return 1 - (sum_denominator / sum_divider)


def get_metric(name: str) -> Metric:
    """
    Factory function to get a metric by name.

    Args:
        name: name of the metric
    Returns:
        Metric: a metric instance corresponding to the given name
    """
    match name:
        case "mean_squared_error":
            return MeanSquaredError()
        case "accuracy":
            return Accuracy()
        case "precision_macro":
            return PrecisionMacro()
        case "recall_macro":
            return RecallMacro()
        case "r_squared":
            return RSquared()
        case "mean_absolute_error":
            return MeanAbsoluteError()
        case _:
            raise KeyError("NO SUCH METRIC FOUND")
